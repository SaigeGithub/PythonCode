{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IDSS Fall 2019 Text Lab.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lsg941010/PythonCode/blob/master/IDSS_Fall_2019_Text_Lab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1suGewMzIKsC",
        "colab_type": "text"
      },
      "source": [
        "# Text processing\n",
        "\n",
        "The aims of the lab are to:\n",
        "*   Learn to perform text processing: tokenization, normalization, and segmentation of text \n",
        "*   Calculate basic collection statistics of a corpus of text\n",
        "*   Introduce the spaCy python library for text processing \n",
        "*   Learn the details of how a *dictionary* is implemented\n",
        "*   Practice creating a sparse one-hot encoding \n",
        "*   Implement Jaccard similarity\n",
        "*   Learn to use SciKit-Learn to vectorize text with a bag-of-words representation\n",
        "*   Use Cosine similarity to find similar documents in a collection\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nKgw7TS5dME",
        "colab_type": "text"
      },
      "source": [
        "## Submission\n",
        "\n",
        "* You must submit the ipynb of this file following the instructions on the [submission page](https://moodle.gla.ac.uk/mod/assign/view.php?id=1409110).\n",
        "\n",
        "* Instead of auto-grading, the marks for this lab are computed using [a Moodle quiz](https://moodle.gla.ac.uk/mod/quiz/view.php?id=1409115).  You will be allowed up to two submissions (in case there is a mistake the first time). \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pq2TfK8Kn04e",
        "colab_type": "text"
      },
      "source": [
        "## Setup \n",
        "\n",
        "#### Your task:####\n",
        "Just run the cells below, and verify that the output is as expected. If anything looks wrong, weird, or crashes, contact the course staff. We don't want library issues to get in the way of the real work! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otJrfV0unyyA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "print(sys.version)\n",
        "\n",
        "# Version checks\n",
        "import importlib\n",
        "def version_check(libname, min_version):\n",
        "    m = importlib.import_module(libname)\n",
        "    print (\"%s version %s is\" % (libname, m.__version__))\n",
        "    print (\"OK\" if m.__version__ >= min_version \n",
        "           else \"out-of-date. Please upgrade!\")\n",
        "    \n",
        "version_check(\"numpy\", \"1.14\")\n",
        "version_check(\"matplotlib\", \"1.6\")\n",
        "version_check(\"pandas\", \"0.22\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "napJywoXLO7u",
        "colab_type": "text"
      },
      "source": [
        "# Load and process Reddit data\n",
        "This lab will be performed on a 'real-world' collection of Reddit posts.  \n",
        "\n",
        "The main unit of processing is a reddit *thread*; it represents a discussion topic with a unique URL. The thread contains metadata as well as the *posts* within it. A *post* is a single user entry in a thread. A post has the *body*, its *author*, it's position in the thread, as well as other metadata. \n",
        "\n",
        "**Thread fields**\n",
        "*   URL - reddit URL of the thread\n",
        "*   title - title of the thread, as written by the first poster\n",
        "*   is_self_post - True if the first post in the thread is a self-post (text addressed to the reddit community as opposed to an external link)\n",
        "*   subreddit - the subreddit of the thread\n",
        "*   posts - a list of all posts in the thread\n",
        "\n",
        "**Post fields**\n",
        "*   id - post ID, reddit ID of the current post\n",
        "*   body - the text of the post\n",
        "*   in_reply_to - parent ID, reddit ID of the parent post, or the post that the current post is in reply to\n",
        "*   post_depth - the number of replies the current post is from the initial post\n",
        "*   is_first_post - True if the current post is the initial post\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hw-TMlAZL7x2",
        "colab_type": "text"
      },
      "source": [
        "Download the Reddit dataset. It's approximately 80 megabytes. It's recommended that this is done in a Colab notebook for this reason. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OyYm3gDwJKQ-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The local location to store the reddit dataset.\n",
        "local_file = \"coarse_discourse_dump_reddit.json\"\n",
        "\n",
        "# The ! performs a shell command to download the reddit dataset from a Google cloud bucket. If you use this\n",
        "# in jupyter directly, there is an equivalent command with wget below.\n",
        "!gsutil cp gs://textasdata/coarse_discourse_dump_reddit.json $local_file\n",
        "  \n",
        "# The ! performs a shell command to download the reddit dataset using wget.\n",
        "#!wget -O  $local_file https://storage.googleapis.com/textasdata/coarse_discourse_dump_reddit.json\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CifGrMcZ5UmZ",
        "colab_type": "text"
      },
      "source": [
        "Note: We will be using the Pandas library as a way to manipulate the data easily.  Please refer to the [documentation](https://pandas.pydata.org/pandas-docs/stable/), or to this [cheat sheet](https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf) if you get stuck. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPyo3mtPvqux",
        "colab_type": "text"
      },
      "source": [
        "Load the JSON data into DataFrame with each post as a row."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-XGj_AFCMBz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The reddit thread structure is nested with posts in a new content.\n",
        "# This block reads the file as json and cates a new data frame.\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "# A temporary variable to store the list of posts.\n",
        "posts_tmp = list()\n",
        "\n",
        "with open(local_file) as jsonfile:\n",
        "  for i, line in enumerate(jsonfile):\n",
        "    thread = json.loads(line)\n",
        "    for post in thread['posts']:\n",
        "      # Keep the thread title and subreddit with each post.\n",
        "      posts_tmp.append((thread['subreddit'], thread['title'], thread['url'],\n",
        "                        post['id'], post.get('author', \"\"), post.get('body', \"\")))\n",
        "# Create the posts data frame.  \n",
        "labels = ['subreddit', 'title', 'url', 'id', 'author', 'body']\n",
        "post_frame = pd.DataFrame(posts_tmp, columns=labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrhtXITZoxXx",
        "colab_type": "text"
      },
      "source": [
        "Let's learn more about the Reddit collection statistics. \n",
        "\n",
        "#### Your Task\n",
        "Use `count()` on the `posts_frame` object to print a count distribution. \n",
        "- Note: `count()` gives count values for each column in the frame independently."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99QhrPH-6tdf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcaIS_vVoVMw",
        "colab_type": "text"
      },
      "source": [
        "This should yield 110,595 for all the fields. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3izMjFyepZ3R",
        "colab_type": "text"
      },
      "source": [
        "#### Your task:\n",
        "Select the subreddit column from the `post_frame` and store it in a variable, `subreddits`.\n",
        "\n",
        "The result is a Series object.  \n",
        "\n",
        "Use the [value_counts](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.value_counts.html) feature to group and count the values for the `subreddits` series.\n",
        "\n",
        "Using the value_counts on the `subreddits` variable do the following:\n",
        " - Print a statistical summary of the data using `.describe() `\n",
        " - Use `head()` to print out the top 5 subreddits with their counts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Mv-ELO66yud",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEV1t9eMsH2L",
        "colab_type": "text"
      },
      "source": [
        "- What information is provided by the describe() function? \n",
        "- What does this statistical summary tell you about the frequency distrubution of threads in subreddits?  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1f-qkoNskE8",
        "colab_type": "text"
      },
      "source": [
        "#### Your task\n",
        "Let's examine the question of how posts relate to threads. Each post has the thread it came from, defined by its URL.  Select the URLs and count the values.  Similar to the previous task, print out the statistical summary using describe.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5is4yyDg60Kd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RI0j_j5vss2b",
        "colab_type": "text"
      },
      "source": [
        "- Critically look at these statistics.  \n",
        "- What is the shortest thread, longest thread?\n",
        "- What about the average?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nsywq_FsOMi",
        "colab_type": "text"
      },
      "source": [
        "### Your task:\n",
        "\n",
        "- Print a simple bar graph to visualize the top 20 most popular sub-reddits by number of posts. \n",
        "\n",
        "**Hint**: Again, the Series object from value_counts has built-in functionality for getting the most frequent as well as plotting, look for \"largest\".  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtXVNlim7EXV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Note: need this for Jupyter on local machines to work\n",
        "%matplotlib inline\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0aDjHtDs4xd",
        "colab_type": "text"
      },
      "source": [
        "#### Your task \n",
        "- Create a bar plot of the top 10 authors of reddit posts.\n",
        "\n",
        "*Note:* Not all posts have authors. As a first step you need to `replace` all empty values in the frame with a numpy nan value `np.nan`, after doing this pandas will filter them out automatically."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6bBykpZSpCt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nczWjc6jLDP_",
        "colab_type": "text"
      },
      "source": [
        "## Introduction to spaCy\n",
        "[spaCy](https://spacy.io/) is an open-source software library for Natural Language Processing, written in Python and Cython. \n",
        "\n",
        "**Note:** SpaCy includes a variety of models. Below we use the english web small. In practice, better effectiveness can be obtained by using a larger model. See the full list of [models](https://spacy.io/usage/models).   "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "070sQ63uTZ7V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python -m spacy download en\n",
        "\n",
        "import spacy\n",
        "import sys\n",
        "print(sys.version)\n",
        "\n",
        "# Version checks\n",
        "import importlib\n",
        "def version_check(libname, min_version):\n",
        "    m = importlib.import_module(libname)\n",
        "    print (\"%s version %s is\" % (libname, m.__version__))\n",
        "    print (\"OK\" if m.__version__ >= min_version \n",
        "           else \"out-of-date. Please upgrade!\")\n",
        "    \n",
        "version_check(\"spacy\", \"2.0\")\n",
        "\n",
        "# Load the small english model. \n",
        "# Disable the advanced NLP features in the pipeline for efficiency.\n",
        "nlp = spacy.load('en_core_web_sm', disable=['ner'])\n",
        "print(nlp.pipeline)\n",
        "print(nlp.pipe_names)\n",
        "nlp.remove_pipe('tagger')\n",
        "nlp.remove_pipe('parser')\n",
        "# Verify they are empty.\n",
        "print(nlp.pipeline)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJ06kvvlKBLG",
        "colab_type": "text"
      },
      "source": [
        "### Step 1: Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hc2ByowtVtD5",
        "colab_type": "text"
      },
      "source": [
        "Below is example code of processing the body of a Reddit post with spaCy. The code below prints out a few of the properties of the [Token](https://spacy.io/api/token) class. This class exposes many useful properties of tokens.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqUx829bVOT1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "doc = nlp(post_frame.loc[10]['body'])\n",
        "for token in doc[:30]:\n",
        "    print(\"{0}\\t{1}\\t{2}\\t{3}\\t{4}\".format(\n",
        "        token.text,\n",
        "        token.idx,\n",
        "        token.lemma_,\n",
        "        token.is_punct,\n",
        "        token.is_space,\n",
        "    ))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfghX7NhMulS",
        "colab_type": "text"
      },
      "source": [
        "Note that spaCy includes the raw token, it's position in the original string, the lemma (using its [lemmatizer](https://spacy.io/api/lemmatizer)), as well other properties of the token. \n",
        "\n",
        "- How does spaCy handle punctuation?\n",
        "- How about splitting based on apostrophes?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qx-e-Nu8KZxO",
        "colab_type": "text"
      },
      "source": [
        "## Tokenization and normalization with spaCy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGv9Pr8wx1dg",
        "colab_type": "text"
      },
      "source": [
        "#### Task\n",
        "- Define a function ``spacy_tokenize`` function that uses spaCy to tokenize a string. The function should:\n",
        " - Accept a string as input\n",
        " - Output a list of spaCy token objects"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bcoe0PGJ3gU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8ualh3vydj2",
        "colab_type": "text"
      },
      "source": [
        "Below we  apply the ``spacy_tokenize`` function to the ``body`` field of the posts in the ``post_frame`` DataFrame. The results are flattened into a ``flat_tokens`` variable that contains a single list of all tokens from all posts concatenated together. \n",
        "\n",
        "Note: Applying spaCy's tokenizer to all the posts will take a couple minutes. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M77rXz2dM-DY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This tokenizes the body posts and creates vector of tokens for each post.\n",
        "# Note: This selects the body column from the posts only. \n",
        "all_posts_tokenized = post_frame.body.apply(spacy_tokenize)\n",
        "\n",
        "import itertools\n",
        "# A single variable with the (flattened) tokens from all posts.\n",
        "flat_tokens = list(itertools.chain.from_iterable(all_posts_tokenized))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEDz3GIN0GEl",
        "colab_type": "text"
      },
      "source": [
        "#### Task\n",
        "\n",
        "- Inspect some of the posts' tokenization to verify that it worked correctly \n",
        "- Print out the 50 most frequent (common) terms in the Reddit collection with their term frequencies (TF). \n",
        "- *Hint*: You'll need to use a property of the token object, not the full token object.\n",
        "\n",
        "Use the python [collections.Counter](https://docs.python.org/2/library/collections.html) library. See it's documentation for examples on how to use it.\n",
        "\n",
        "A `Counter` is a container that stores elements as dictionary keys, and their counts are stored as dictionary values.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uoAihiIcJy1q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q56frVIwT9U8",
        "colab_type": "text"
      },
      "source": [
        "You may observe that some of the tokens are 'noisy'. Some may be line breaks as well as various other types of punctuation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOWgvXpjuCk-",
        "colab_type": "text"
      },
      "source": [
        "### Step 2: Text Normalization\n",
        "\n",
        "In this section we will apply simple text normalization. We will write a function that takes raw tokens and normalizes them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0mp6eapy2LK",
        "colab_type": "text"
      },
      "source": [
        "#### Task:\n",
        "Create a ``normalize`` function that normalizes raw text into a canonical form. The function should:\n",
        " - Take a list of spaCy token objects as input\n",
        " - Output a list of normalized strings\n",
        "\n",
        "Simple Normalization algorithm:\n",
        " - Normalization should only keep tokens consisting of alphanumeric characters.    (Hint: See Spacy's [token object](https://spacy.io/api/token) documentation for character classes.) \n",
        " - Normalization should use Spacy's lemma property for the word representation. \n",
        " - Output should be lowerecased \n",
        " - Extra whitespace on the ends should be removed. \n",
        " - One edge case to handle is when Spacy's lemma is \"-PRON-\". In this case, preserve the lowercased text token instead.\n",
        " \n",
        "\n",
        "The follow-up answers depend on this code being correct. Once you've written your version, please check it with the code below.  Click SHOW CODE to see it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WfRcbOJOy6j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14m7RfqGzo_x",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "def normalize(tokens):\n",
        "  normalized = list()\n",
        "  for token in tokens:\n",
        "    if (token.is_alpha):\n",
        "      lemma = token.lemma_.lower().strip() if token.lemma_ != \"-PRON-\" else token.lower_\n",
        "      normalized.append(lemma)\n",
        "  return normalized"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWDK5Fwi6ByI",
        "colab_type": "text"
      },
      "source": [
        "The code below runs the ``normalize`` function on the ``flat_tokens`` and stores it in ``normalized_tokens``. We will use these for our vocabulary and processing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_D_wpV5Q4l69",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "normalized_tokens = normalize(flat_tokens)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1JtYDNauOtC",
        "colab_type": "text"
      },
      "source": [
        "#### Your task\n",
        "Fill in the blanks in the code below to compute the statistics for the collection. \n",
        "\n",
        "\n",
        "*Hint:* Python has built-in `set()` data structure to create unique the values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCqnopUFuQgf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# |N| - total number of raw unnormalized tokens (Hint: recall flat_tokens)\n",
        "N_raw = YOUR CODE\n",
        "\n",
        "# Average post length in raw, unnormalized, tokens\n",
        "average_raw_token_length = YOUR CODE\n",
        "\n",
        "# Size of raw (unnormalized) unique tokens\n",
        "B = YOUR CODE\n",
        "\n",
        "# Set of unique normalized tokens (from normalized_tokens) --> the vocabulary\n",
        "V = YOUR CODE\n",
        "\n",
        "\n",
        "print(N)\n",
        "print(average_raw_token_length)\n",
        "\n",
        "print(B)\n",
        "print(V)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0k8ot06euc0g",
        "colab_type": "text"
      },
      "source": [
        "- Hint: N should between 5-6 million tokens. \n",
        "- B is approximately 150k unique raw tokens \n",
        "\n",
        "Normalization matters -- the vocabulary is roughly half the number of unique unnormalized tokens.\n",
        "\n",
        "#### Optional task\n",
        "- On all_posts_tokenized compute the length of each token in characters.  Consider using `describe()` to compute the full suite of statistics. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhZingDUuQn5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGXn8xkgTn6O",
        "colab_type": "text"
      },
      "source": [
        "## One-hot encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JBfGbpYBO8N",
        "colab_type": "text"
      },
      "source": [
        "We will now implement a one-hot encoding text representation using a dictionary.\n",
        "\n",
        "Below is a skeleton class that implements a dictionary.  Recall from lecture that a dictionary allows us to translate a series of tokens to integer values (and back).\n",
        "\n",
        "#### Your task\n",
        "- The ``SimpleDictionary`` skeleton below is incomplete, fill in the missing elements. Specifically:  \n",
        " - Complete the ``_init_`` constructor to initialize all the member variables appropriately from the token input\n",
        " - Implement the ``tokens_to_ids`` function that maps strings to integer values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAVcr270_gz2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class SimpleDictionary(object):\n",
        "  \n",
        "  # Special UNK token for unseen tokens\n",
        "  UNK_TOKEN = \"<unk>\"\n",
        "\n",
        "  def __init__(self, tokens, size=None):\n",
        "    \n",
        "    # Map of unigram tokens to their count.\n",
        "    # Hint: Recall the Counter object.\n",
        "    self.unigram_counts = \n",
        "    \n",
        "    # The size of the vocabulary, |V|.\n",
        "    # Hint: Consider using the data structure containing the unigram_counts.\n",
        "    self.vocabulary_size = \n",
        "\n",
        "    # The total number of tokens in the collection, |N|\n",
        "    self.collection_size = \n",
        "\n",
        "    # Vocabulary - most frequent unique terms \n",
        "    # (Note: This is limited by the *size* parameter passed in.)\n",
        "    # These should be in descending order of collection frequency.\n",
        "    # Remember to add the \"<unk>\" token. \n",
        "    # It should go first in the ordering. Why might this be the case?\n",
        "    self.vocab = \n",
        "\n",
        "\n",
        "    # Dictionary that assigns an id to each token, based on frequency.\n",
        "    # Hint: use a dictionary data structure with the vocab.\n",
        "    self.id_to_token = \n",
        "    \n",
        "    # Dictionary that assigns a token to its id\n",
        "    # Hint: The reverse of the id_to_token dictionary. \n",
        "    self.token_to_id = \n",
        "    \n",
        "    self.size = len(self.id_to_token)\n",
        "    if size is not None:\n",
        "        assert(self.size <= size)\n",
        "\n",
        "    # For convenience keep a set of unique words.\n",
        "    self.tokenset = set(iter(self.token_to_id.keys()))\n",
        "\n",
        "    # Store special IDs for convenience\n",
        "    self.UNK_ID = self.token_to_id[self.UNK_TOKEN]\n",
        "\n",
        "  # Given a sequence of ids, return a sequence of corresponding tokens.\n",
        "  def ids_to_tokens(self, ids):\n",
        "    return [self.id_to_token[i] for i in ids]\n",
        "  \n",
        "  # Given an input sequence of tokens, return a sequence of token id.\n",
        "  # This performs the vectorizing of text.\n",
        "  def tokens_to_ids(self, tokens):\n",
        "    # YOUR CODE HERE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVeVMgT3YthP",
        "colab_type": "text"
      },
      "source": [
        "Run the dictionary on the ``normalized_tokens`` that contains all of the tokens in the collection.  In Sci-kit Learn this is called \"fitting\", creating a vocabulary from a fixed collection of text. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BX_LjcgQwvk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dictionary = SimpleDictionary(normalized_tokens)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1f3rt_fQ2Bw",
        "colab_type": "text"
      },
      "source": [
        "#### Verification task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qb_B4Wa76J6P",
        "colab_type": "text"
      },
      "source": [
        "- Use the ``dictionary`` to print out properties of the text collection. \n",
        "\n",
        " - Print out the total number of tokens (N)\n",
        " - Print out the size of the vocabulary  (V)\n",
        " - Print out the top 20 most frequent unigrams with three values: token, collection frequency, percentage of collection tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7G8PIKOKTEkv",
        "colab_type": "text"
      },
      "source": [
        "The vocabulary size should match |V| from above. \n",
        "In this case the normalized |N| will be smaller than raw token counts -- approximately 4.5M normalized tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YMljTTdwtMI",
        "colab_type": "code",
        "outputId": "51d47608-418a-497a-9847-a30f4baf26d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "#@title\n",
        "print(\"Collection size: \" + \"{0}\".format(dictionary.collection_size))\n",
        "print(\"Vocabulary size: \" + \"{0}\".format(dictionary.vocabulary_size))\n",
        "\n",
        "for (word, count) in dictionary.unigram_counts.most_common(20):\n",
        "  print(\"{0}\\t{1}\\t{2}\".format(word, count, 100 * count / dictionary.collection_size))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collection size: 4449028\n",
            "Vocabulary size: 72368\n",
            "the\t175936\t3.954481742978466\n",
            "be\t167894\t3.773723159305808\n",
            "a\t149416\t3.358396485704293\n",
            "i\t146584\t3.2947421324388158\n",
            "to\t124878\t2.8068602849880917\n",
            "and\t102055\t2.29387182998174\n",
            "it\t83701\t1.8813322820175553\n",
            "you\t78023\t1.7537089000114183\n",
            "of\t73378\t1.6493040727098143\n",
            "that\t66063\t1.4848861369269872\n",
            "in\t56212\t1.2634669864968258\n",
            "have\t56017\t1.2590840066639275\n",
            "for\t48242\t1.0843267338393914\n",
            "do\t48227\t1.083989581544553\n",
            "on\t34400\t0.7732025961625776\n",
            "but\t34350\t0.7720787551797831\n",
            "with\t33157\t0.7452639093303076\n",
            "this\t32614\t0.73305899625716\n",
            "can\t31966\t0.718494017120144\n",
            "my\t29458\t0.6621221534231747\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WF0X7aqS8Yz1",
        "colab_type": "text"
      },
      "source": [
        "The most frequent word, *the*, accounts for approximately 4% of all tokens.  The top 10 most frequent words account for over 25% of all word occurrences.  Recall [Zipf's law](https://simple.wikipedia.org/wiki/Zipf%27s_law) and the power law distribution of text data. A few number of terms account for a large fraction of occurrences, but many words occur rarely. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElIuBpMaSJ5X",
        "colab_type": "text"
      },
      "source": [
        "### From tokens to IDs and back again\n",
        "Below are some examples of using the dictionary to map tokens to IDs in our vocabulary (and vice versa). Consider trying some of your own words to experiment with what happens here. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_t2NjsIOpSR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Pick a word from the dictionary\n",
        "print(dictionary.tokens_to_ids([\"a\"]))\n",
        "\n",
        "# What's the value of a made up word? \n",
        "print(dictionary.tokens_to_ids([\"uphalyday\"]))\n",
        "\n",
        "# For fun, let's print out a couple random words from the vocab.\n",
        "# Feel free to explore the vocabulary.\n",
        "import random as rand\n",
        "print(dictionary.ids_to_tokens([5]))\n",
        "print(dictionary.ids_to_tokens([rand.randint(0, dictionary.size-1)]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md2M0HVgSe_s",
        "colab_type": "text"
      },
      "source": [
        "### Creating a one-hot encoding representation\n",
        "\n",
        "#### Your task\n",
        "- Create a function: ``one_hot_encoding`` that uses the ``SimpleDictionary`` to take a string and return a vector of integers:\n",
        " - Takes a string as input and applies tokenization and normalization using the provided ``tokenize_normalize`` function.\n",
        " - Output a sparse one-hot encoding of the text (sorted in ascending term id order). \n",
        " - Test the code by running it on the post in row index 10 in the post_frame.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZCnLAjhGJRo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize_normalize(string):\n",
        "  return normalize(spacy_tokenize(string))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3hh-n1LVyaF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLXL62OnRVeo",
        "colab_type": "code",
        "outputId": "94c1f525-a479-466a-cc77-99fd0cb129e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "\n",
        "post = post_frame.loc[10]['body']\n",
        "print(len(spacy_tokenize(post)))\n",
        "print(one_hot_encoding(post))\n",
        "print(len(one_hot_encoding(post)))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "111\n",
            "[1, 2, 3, 4, 5, 6, 7, 10, 12, 13, 14, 19, 21, 23, 26, 27, 35, 39, 46, 54, 57, 63, 65, 67, 89, 93, 98, 105, 132, 152, 166, 179, 223, 239, 240, 271, 272, 321, 341, 352, 412, 458, 533, 537, 720, 739, 1109, 1339, 1484, 1552, 1575, 1798, 1883, 1996, 2269, 2838, 3231, 4087, 4810, 18070, 38717, 38718]\n",
            "62\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qjeSYIPW5km",
        "colab_type": "text"
      },
      "source": [
        "The original input sequence for the post has 111 raw tokens. The one-hot encoding should be a list of with 62 values (including all of tokens 1-7). \n",
        "\n",
        "A sparse one-hot encoding is common and is easily to compress."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XI2IHWEUVqtM",
        "colab_type": "text"
      },
      "source": [
        "Try creating a one-hot encoding of your favorite sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUQOJDvWx95N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98bwvM1HUQSD",
        "colab_type": "text"
      },
      "source": [
        "## Jaccard similarity betwen pieces of text\n",
        "\n",
        "#### Your task \n",
        "\n",
        "- Implement a function ``jaccard_similarity`` that takes two documents represented as sparse one-hot encodings and computes the jaccard similarity. \n",
        "- *Hint*: You might want to look at the operations on the built-in set datastructure \n",
        "(https://docs.python.org/3/tutorial/datastructures.html#sets)\n",
        "- *Debugging Tip*: Consider printing the different elements of Jaccard. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJ1dn3KZV9r2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APOVSEndauri",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "doc1 = one_hot_encoding(\"the cat jumped over the fox\")\n",
        "doc2 = one_hot_encoding(\"the brown fox jumped over the dog\")\n",
        "\n",
        "print(doc1)\n",
        "print(doc2)\n",
        "jaccard_similarity(doc1, doc2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQT8PaaIdqMj",
        "colab_type": "text"
      },
      "source": [
        "Implementing other one-hot encoding similarity measures should be easy for you to do now. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKnOHrlEfOjj",
        "colab_type": "text"
      },
      "source": [
        "### Section summary\n",
        "In the previous section we: \n",
        " - Created a dictionary object and used it to represent text. \n",
        " - Created a one-hot encoding of text documents\n",
        " - Implemented the Jaccard similarity function \n",
        "\n",
        "\n",
        "We could also have extended our functions to create bag-of-words representations.  In the next section we'll explore how to do this with one of the most widely used machine learning libraries. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XWjf3yuTO6J",
        "colab_type": "text"
      },
      "source": [
        "## Vector representations with Scikit-Learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25XYnTCi2CWz",
        "colab_type": "text"
      },
      "source": [
        "Scikit-learn is a widely machine learning library that includes tools for performing operations on data: similarity computation, clustering, classification, and many others. You'll now use Scikit-learn to create vector representations of text.\n",
        "\n",
        "We first extract out a few fields from the DataFrame and put it into a friendlier format. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhRAtCCvaDpw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from itertools import islice\n",
        "\n",
        "# Parallel arrays of the post keys and values.\n",
        "post_vals = list()\n",
        "post_keys = list()\n",
        "\n",
        "# We limit the size of the data loaded to the first 5000 posts to speed processing.\n",
        "posts_to_load = 5000\n",
        "\n",
        "for post in islice(post_frame.itertuples(index=True, name='Pandas'), posts_to_load):\n",
        "    post_keys.append(getattr(post, 'id'))\n",
        "    post_vals.append(getattr(post,'body'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjc1wcH8f-Xo",
        "colab_type": "text"
      },
      "source": [
        "#### Your task\n",
        "Create a document-term matrix with term frequency (counts) from the Reddit posts using the [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html). This creates a bag-of-words representation for a document. \n",
        "\n",
        "The code will have the following steps:\n",
        " - Import the ``CountVectorizer`` and create an instance; assign it to a variable, ``tf_vectorizer``\n",
        " - All Scikit-Learn vectorizers accept a tokenizer as an optional parameter (it has a built-in tokenizer).  Pass in the ``tokenize_normalize`` function created earlier by using ``CountVectorize (tokenizer=...)``. This tokenizes and normalizes with spaCy. \n",
        " - Call ``fit`` on the ``post_vals`` variable to 'learn' the vocabulary on the collection, similar to the constructor in SimpleDictionary.\n",
        " - Call ``transform`` on the ``post_vals`` to create a document-term matrix, assign it to a variable, `tf_document_term_matrix`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dohB_3uSbKR9",
        "colab_type": "text"
      },
      "source": [
        "Note: Running this processing on the whole collection make take approximately 5 minutes in a colab instance. The small dataset should take a few seconds."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XJA3JDj2Hid",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppFI17VRiA56",
        "colab_type": "text"
      },
      "source": [
        "What did this process do?\n",
        " - The ``fit()`` function tokenized the text collection and built a vocabulary/dictionary \n",
        " - The ``transform()`` function created a document-term matrix with a bag-of-words representation (raw TF) word counts as the weighting.\n",
        " \n",
        " These steps are sometimes combined together with the single step called ``fit_transform``.  The constructor of ``CountVectorizer`` accepts parameters on how to control the dictionary created. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlZXjw0Fiif8",
        "colab_type": "text"
      },
      "source": [
        "Let's now apply the vectorizer on new unseen text.  We do this by calling ``transform()`` on the string data (technically an array of strings, each entry a document). \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4p0tn37ZW1Ji",
        "colab_type": "code",
        "outputId": "6ef9e290-6058-4fc5-b70c-7974a462e98e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "mystring = 'The next town over recently got a brand new flagship Lidl – now my town is getting a brand new flagship Aldi. I fear war.'\n",
        "response = tf_vectorizer.transform([mystring])\n",
        "print (response)\n",
        "print (tf_vectorizer.inverse_transform(response))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  (0, 4)\t2\n",
            "  (0, 808)\t1\n",
            "  (0, 1079)\t2\n",
            "  (0, 3115)\t1\n",
            "  (0, 3545)\t2\n",
            "  (0, 4133)\t1\n",
            "  (0, 5537)\t1\n",
            "  (0, 5656)\t2\n",
            "  (0, 5667)\t1\n",
            "  (0, 5762)\t1\n",
            "  (0, 6015)\t1\n",
            "  (0, 6891)\t1\n",
            "  (0, 8447)\t1\n",
            "  (0, 8629)\t2\n",
            "  (0, 9174)\t1\n",
            "[array(['a', 'be', 'brand', 'fear', 'get', 'i', 'my', 'new', 'next', 'now',\n",
            "       'over', 'recently', 'the', 'town', 'war'], dtype='<U41')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0Q0vgPe3dqD",
        "colab_type": "text"
      },
      "source": [
        "This outputs the term-doc matrix with term frequency values. The output has several parts:  \n",
        " - (0, 17) 2 --> (doc row, column) term_frequency\n",
        " \n",
        " \n",
        " What happened? \n",
        " - ``Transform`` applies the vectorizor, just like ``tokens_to_ids`` in the dictionary implementation.  \n",
        " - The result is a document-term matrix for the data passed to it. \n",
        " \n",
        " The ``inverse_transform`` is just like ``ids_to_tokens`` applied to every non-UNK value (which are not invertable).  \n",
        "\n",
        "**Note:** We do not call ``fit`` on `mystring`.  Why? What would this have done?\n",
        "\n",
        "In the inverse output,  see that some of the words are not present (e.g. lidl, aldi, etc...). \n",
        "\n",
        "These are just ignored; calling ``transform()`` does not update our vocabulary and there is no UNK representation in its vocabulary. This is important -- Scikit-Learn blindly ignores all UNK tokens that haven't been seen when using a vectorizer; be careful. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXmMp1CTWaWi",
        "colab_type": "text"
      },
      "source": [
        "#### Your task \n",
        "- Create a TF-IDF vectorizer, name it `tfidf_vectorizer` and use it to vectorize ``post_vals`` similar to what was done for CountVectorizer. \n",
        " - Use the [TfidfVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html).\n",
        " - Set `sublinear_tf` = True to use the log scaling (as opposed to raw TF counts)\n",
        " - Limit the number of features (the size of the vocabulary) to 50000\n",
        " - Assign the resulting document-term matrix to a variable: `tfidf_matrix` \n",
        " \n",
        "Read the documentation of the vectorizer for details on the parameters as needed.  Warning: if this takes too long (e.g. more than 5 minutes or so), or crashes then your configuration is wrong.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15lT97-M2GTM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEplJqQ4Zl4l",
        "colab_type": "text"
      },
      "source": [
        "## Cosine similarity\n",
        "We will now use sklearn's `cosine_similarity` implementation to compute the cosine similarity between all posts, see the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7uWZv0clzms",
        "colab_type": "text"
      },
      "source": [
        "**Task:** Import the `cosine_similarity` function.\n",
        "- Create a cosine similarity matrix of all pairs of documents.\n",
        "- Use the `tfidf_matrix` as the input.\n",
        "- Print the resulting cosine similarity matrix.\n",
        "- Find the maximum cosine similarity for the first document (first row), excluding itself (first column)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBVr-s6u3pVb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cD0py8rgsxy9",
        "colab_type": "text"
      },
      "source": [
        "You should see that there are 1s across the diagonal of the matrix as the similarity of a post to itself is 1. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W66BwN2Z7dPc",
        "colab_type": "text"
      },
      "source": [
        "For your own enrichment you can use this as a basis to experiment with different vectorizers and matrix representations (count, tfidf, ngrams). For example, also removing stopwords or other non-informative words\n",
        "- How do the post similarities change?\n",
        "- How could you measure if this effective?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rW03I8Kn0KCq",
        "colab_type": "text"
      },
      "source": [
        "## Summary \n",
        "\n",
        "In this lab we covered the fundamentals of texting processing:\n",
        "- We introduced spaCy for text processing\n",
        "- You implemented a dictionary and created a one-hot encoding of text\n",
        "- Implemented the Jaccard similarity function\n",
        "- Used Sci-kit Learn to vectorize text using bag-of-words representation with TF and TF-IDF weights \n",
        "- Computed the cosine similarity between Reddit posts"
      ]
    }
  ]
}